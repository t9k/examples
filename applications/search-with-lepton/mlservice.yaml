apiVersion: tensorstack.dev/v1beta1
kind: MLService
metadata:
  name: search-with-lepton-vllm
spec:
  # scheduler:
  #   t9kScheduler:
  #     queue: default        # 指定队列 
  default: version1
  releases:
    - name: version1
      predictor:
        minReplicas: 1        # 副本数量
        containersResources:
          - name: user-container
            resources:
              limits:
                cpu: 2
                memory: 8Gi
        model:
          runtime: search-with-lepton
        template:
          spec:
            containers:
              - name: user-container
                env:
                - name: BACKEND            # 使用的搜索引擎后端
                  value: "BING"            # 应为 "BING"、"GOOGLE"、"SERPER" 或 "SEARCHAPI"
                - name: BING_SEARCH_V7_SUBSCRIPTION_KEY
                  value: ""                              # 对于 BING 后端必需
                - name: GOOGLE_SEARCH_API_KEY
                  value: ""                              # 对于 GOOGLE 后端必需
                - name: GOOGLE_SEARCH_CX
                  value: ""                              # 对于 GOOGLE 后端必需
                - name: SERPER_SEARCH_API_KEY
                  value: ""                              # 对于 SERPER 后端必需
                - name: SEARCHAPI_API_KEY
                  value: ""                              # 对于 SEARCHAPI 后端必需
                - name: OPENAI_BASE_URL    # OpenAI API 服务器（或与其兼容的服务器）的 URL
                  value: "http://mixtral-8x7b.demo/v1"
                                           # 应为 "<ADDRESS>/v1"，其中 <ADDRESS> 为 vLLM 推理服务的地址, 可通过以下命令获取：
                                           # `kubectl get mlservice <VLLM_MLSERVICE_NAME> -ojsonpath='{.status.address.url}'`
                - name: OPENAI_API_KEY
                  value: "any"
                - name: LLM_MODEL          # 使用的模型名称
                  value: "mixtral-8x7b"    # 应为部署 vLLM 推理服务时为 `--served-model-name` 参数设定的值
                - name: RELATED_QUESTIONS  # 是否生成关联问题并展示
                  value: "false"           # 暂不支持
