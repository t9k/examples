apiVersion: batch.tensorstack.dev/v1beta1
kind: DeepSpeedJob
metadata:
  name: baichuan2-dpo
spec:
  # scheduler:
  #   t9kScheduler:
  #     queue: default
  #     priority: 50
  config:
    slotsPerWorker: 4
    run:
      python:
        - ./src/train_bash.py
        - "--deepspeed=/t9k/mnt/examples/llama-efficient-tuning/ds-config.json"
        - "--stage=dpo"
        - "--model_name_or_path=/t9k/mnt/models/Baichuan2-7B-Base"
        - "--do_train"
        - "--dataset=comparison_gpt4_zh"
        - "--template=default"
        - "--finetuning_type=lora"
        - "--lora_target=W_pack"
        - "--resume_lora_training=False"
        - "--checkpoint_dir=/t9k/mnt/output/sft-ckpts/baichuan2/7b/"
        - "--output_dir=/t9k/mnt/output/dpo-ckpts/baichuan2/7b"
        - "--per_device_train_batch_size=2"
        - "--gradient_accumulation_steps=4"
        - "--lr_scheduler_type=cosine"
        - "--logging_steps=10"
        - "--save_steps=400"
        - "--learning_rate=1e-5"
        - "--num_train_epochs=1.0"
        - "--plot_loss"
        - "--bf16"
  worker:
    replicas: 1
    template:
      spec:
        containers:
          - name: worker
            workingDir: /t9k/mnt/LLaMA-Efficient-Tuning
            image: t9kpublic/llama-efficient-tuning:20230918
            securityContext:
              capabilities:
                add: [ "IPC_LOCK" ]
            resources:
              requests:
                cpu: 8
                memory: 128Gi
                nvidia.com/gpu: 4
              limits:
                cpu: 16
                memory: 256Gi
                nvidia.com/gpu: 4
            volumeMounts:
              - mountPath: /t9k/mnt
                name: data
              - mountPath: /dev/shm
                name: dshm
        volumes:
          - name: data
            persistentVolumeClaim:
              claimName: llama-efficient-tuning
          - name: dshm
            emptyDir:
              medium: Memory
