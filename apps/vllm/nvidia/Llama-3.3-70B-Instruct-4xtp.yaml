server:
  image:
    registry: "$(T9K_APP_IMAGE_REGISTRY)"
    repository: "$(T9K_APP_IMAGE_NAMESPACE)/vllm-openai"
    tag: "v0.8.0"
    pullPolicy: IfNotPresent

  resources:
    limits:
      cpu: 8
      memory: 64Gi
      nvidia.com/gpu: 4

  model:
    deployName: "Llama-3.3-70B-Instruct"

    volume:
      storageClass: ""
      size: 32Gi
      accessModes:
        - ReadWriteOnce
      existingClaim: "llm"
      subPath: "Llama-3.3-70B-Instruct/"
  
  autoScaling:
    minReplicas: 1
    maxReplicas: 1

  app:
    extraArgs:
      - "--tensor-parallel-size=4"
      - "--gpu-memory-utilization=0.95"
      - "--max-model-len=8192"
      - "--max-num-seqs=64"
      - "--enforce-eager"

initializer:
  image:
    registry: "$(T9K_APP_IMAGE_REGISTRY)"
    repository: "$(T9K_APP_IMAGE_NAMESPACE)/kubectl"
    tag: "1.27"
    pullPolicy: IfNotPresent
