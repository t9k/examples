apiVersion: batch.tensorstack.dev/v1beta1
kind: PyTorchTrainingJob
metadata:
  name: llama3-1-70b-sft-4xdp-4bit
spec:
  # scheduler:
  #   t9kScheduler:
  #     queue: default
  #     priority: 50
  torchrunConfig:
    enabled: true
    maxRestarts: 3
    procPerNode: "4"
    rdzvBackend: c10d
  replicaSpecs:
    - type: worker
      replicas: 1
      restartPolicy: OnFailure
      template:
        spec:
          containers:
            - name: worker
              image: t9kpublic/llama-factory:20240730-2
              workingDir: /workspace
              args: 
                - LLaMA-Factory/src/llamafactory/launcher.py
                - examples/llama-factory/training/sft-70b-4xdp-4bit-config.yaml
              env:
                - name: HF_ENDPOINT
                  value: "https://hf-mirror.com"
              resources:
                requests:
                  cpu: 8
                  memory: 32Gi
                  nvidia.com/gpu: 4
                limits:
                  cpu: 16
                  memory: 64Gi
                  nvidia.com/gpu: 4
              volumeMounts:
                - mountPath: /workspace
                  name: data
                - mountPath: /dev/shm
                  name: dshm
          volumes:
            - name: data
              persistentVolumeClaim:
                claimName: llama-factory
            - name: dshm
              emptyDir:
                medium: Memory
