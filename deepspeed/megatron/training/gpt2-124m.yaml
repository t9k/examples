apiVersion: batch.tensorstack.dev/v1beta1
kind: PyTorchTrainingJob
metadata:
  name: gpt2-124m
spec:
  # runMode:
  #   debug:
  #     enable: true
  #     replicaSpecs:
  #       - type: worker
  #         skipInitContainer: true
  #         command: ["sleep", "inf"]
  scheduler:
    t9kScheduler:
      queue: mlperf
      priority: 50
  torchrunConfig:
    enable: true
    maxRestarts: 3
    procPerNode: "1"
    rdzvBackend: c10d
  replicaSpecs:
    - type: node
      replicas: 1
      restartPolicy: ExitCode
      template:
        spec:
          containers:
            - name: pytorch
              args:
                - Megatron-LM/pretrain_gpt.py
                - --num-layers 12
                - --hidden-size 768
                - --num-attention-heads 12
                - --seq-length 1024
                - --max-position-embeddings 1024
                - --micro-batch-size 4
                - --global-batch-size 8
                - --lr 0.00015
                - --train-iters 500000
                - --lr-decay-iters 320000
                - --lr-decay-style cosine
                - --min-lr 1.0e-5
                - --weight-decay 1e-2
                - --lr-warmup-fraction .01
                - --clip-grad 1.0
                - --fp16
                - --data-path examples/deepspeed/megatron/dataset/wiki-en/gpt_text_document
                - --vocab-file examples/deepspeed/megatron/tokenizer/wiki-en-tokenizer/vocab.json
                - --merge-file examples/deepspeed/megatron/tokenizer/wiki-en-tokenizer/merges.txt
                - --data-impl mmap
                - --split 949,50,1
                - --log-interval 100
                - --save-interval 10000
                - --eval-interval 1000
                - --eval-iters 10
                - --save output/gpt2-124m
                - --load output/gpt2-124m
              workingDir: /t9k/mnt/
              image: tsz.io/t9k/nvidia-pytorch:23.05-py3
              securityContext:
                capabilities:
                  add: [ "IPC_LOCK" ]
              resources:
                requests:
                  cpu: 2
                  memory: 16Gi
                  nvidia.com/gpu: 1
                limits:
                  cpu: 4
                  memory: 32Gi
                  nvidia.com/gpu: 1
              volumeMounts:
                - mountPath: /t9k/mnt
                  name: data
                - mountPath: /dev/shm
                  name: dshm  
          volumes:
            - name: data
              persistentVolumeClaim:
                claimName: megatron
            - name: dshm
              emptyDir:
                medium: Memory
