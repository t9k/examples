apiVersion: tensorstack.dev/v1beta1
kind: MLService
metadata:
  name: codellama-70b
spec:
  # scheduler:
  #   t9kScheduler:
  #     queue: default
  default: v1
  releases:
    - name: v1
      predictor:
        minReplicas: 1
        model:
          runtime: vllm-openai-2xtp
          parameters:
            MODEL_PATH: /var/lib/t9k/model
            MODEL_NAME: codellama-70b
        containersResources:
        - name: user-container
          resources:
            limits:
              cpu: 4
              memory: 64Gi
              nvidia.com/gpu: 2
        storage:
          pvc:
            name: vllm
            subPath: CodeLlama-70b-Instruct-hf