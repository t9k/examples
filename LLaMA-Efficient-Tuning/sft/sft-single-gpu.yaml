apiVersion: batch.tensorstack.dev/v1beta1
kind: DeepSpeedJob
metadata:
  name: baichuan-sft-single-gpu1
spec:
  runMode:
    debug:
      enable: true
      replicaSpecs:
        - type: worker
          skipInitContainer: true
          command: ["sleep", "inf"]
  scheduler:
    t9kScheduler:
      queue: mlperf
      priority: 50
  config:
    slotsPerWorker: 1
    run:
      python:
        - ./src/train_bash.py
        - "--deepspeed=/t9k/mnt/ds-config.json"
        - "--stage=sft"
        - "--model_name_or_path=/t9k/mnt/models/Baichuan-7B"
        - "--do_train"
        - "--dataset=alpaca_gpt4_en"
        - "--template=default"
        - "--lora_target=W_pack"
        - "--finetuning_type=lora"
        - "--output_dir=/t9k/mnt/output/sft-models/baichuan/7b/"
        - "--overwrite_cache"
        - "--per_device_train_batch_size=8"
        - "--gradient_accumulation_steps=2"
        - "--lr_scheduler_type=cosine"
        - "--logging_steps=10"
        - "--save_steps=3300"
        - "--learning_rate=5e-5"
        - "--num_train_epochs=3.0"
        - "--plot_loss"
        - "--bf16"
  worker:
    replicas: 1
    template:
      spec:
        containers:
          - name: worker
            workingDir: /t9k/mnt/LLaMA-Efficient-Tuning
            image: tsz.io/xyx/llama-efficient-tuning:20230905
            securityContext:
              capabilities:
                add: [ "IPC_LOCK" ]
            resources:
              requests:
                cpu: 2
                memory: 32Gi
                nvidia.com/gpu: 1
              limits:
                cpu: 4
                memory: 64Gi
                nvidia.com/gpu: 1
            volumeMounts:
              - mountPath: /t9k/mnt
                name: data
              - mountPath: /dev/shm
                name: dshm
        volumes:
          - name: data
            persistentVolumeClaim:
              claimName: llama
          - name: dshm
            emptyDir:
              medium: Memory
